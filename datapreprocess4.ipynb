{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5LJx-ZZcGjs"
   },
   "source": [
    "### 对用户行为数据进行预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPJwb0fzVz2_"
   },
   "source": [
    "    这篇 notebook 的主要作用是根据用户的历史行为信息进行用户画像,主要是挖掘用户的兴趣信息，然后分开为训练集和测试集输入到树模型中进行；\n",
    "    具体方式为: \n",
    "      根据前7天的历史行为数据，对用户和物品进行进一步地描述和挖掘（也可以之后尝试不是7天的历史行为数据，而是根据所有历史行为数据进行挖掘）\n",
    "      然后将挖掘到的信息，作为第8天的特征，将第八天的数据输入到树模型中进行训练；\n",
    "      数据集一共包含了14天的数据，于是可以分别划分为8份，分别是：\n",
    "      1. 挖掘 1 到 7 天的信息，作为第 8 天数据的特征；\n",
    "      2. 挖掘 2 到 8 天的信息，作为第 9 天数据的特征；\n",
    "      3. 挖掘 3 到 9 天的信息，作为第 10 天数据的特征；\n",
    "      4. 挖掘 4 到 10 天的信息，作为第 11 天数据的特征；\n",
    "      5. 挖掘 5 到 11 天的信息，作为第 12 天数据的特征；\n",
    "      6. 挖掘 6 到 12 天的信息，作为第 13 天数据的特征；\n",
    "      7. 挖掘 7 到 13 天的信息，作为第 14 天数据的特征；\n",
    "      8. 挖掘 8 到 14 天的信息，作为第 15 天数据的特征；\n",
    "      在线下训练阶段，选在第 8 天到第 13 天的数据作为训练集，而第 14 天的数据做验证；\n",
    "      当需要test数据进行预测时候，则直接输入第 8 天到第 14 天的数据进行训练，而test是第 15 天的数据；\n",
    "\n",
    "    需要挖掘的历史行为数据包括以下几个部分：\n",
    "    1.用户兴趣挖掘，根据用户过去7天的行为数据，挖掘用户对于feed的偏好信息，然后将这些偏好信息或者直接输入模型，或者和对应的feed特征进行一些交叉；\n",
    "      当前视频的特征包括：authorid、videoplaysecond、bgm_song_id、bgm_singer_id、author_n_feeds、top5_keywords、top5_keywords_weights、tag_list、tag_weights、feed_embedding\n",
    "      对于这些特征，可以依次挖掘出一些信息：\n",
    "      authorid：挖掘此信息主要是挖掘出用户对于author的偏好，主要可以挖掘的包括：\n",
    "      ·观看历史中，authorid出现的次数；\n",
    "      ·观看历史中，authorid所占的比例；\n",
    "      ·观看历史中，authorid出现的次数占author作者所有作品的比例\n",
    "      ·用户所有完整播放甚至重复播放的视频中，authorid出现的次数；\n",
    "      ·用户所有完整播放甚至重复播放的视频中，authorid所占的比例；\n",
    "      ·用户所有完整播放甚至重复播放的视频中，authorid占authorid出现次数的比例；\n",
    "      ·用户停留时间很长的视频中，authorid出现的次数；\n",
    "      ·用户停留时间很长的视频中，authorid所占的比例；\n",
    "      ·用户停留时间很长的视频中，authorid占authorid出现次数的比例\n",
    "      ·某一具体的历史action中，authorid出现的次数；\n",
    "      ·某一具体的历史action中，authorid所占的比例；\n",
    "      ·某一具体的历史action中，authorid占authorid出现次数的比例\n",
    "      ·对于和某一action强相关的特征，也按照上面3个进行统计；\n",
    "        - click_avatar 统计 follow \n",
    "        - like 统计 follow、comment\n",
    "        - read_comment 统计 favorite、comment\n",
    "      以上挖掘出的authorid特征都需要和视频的authorid做交叉，也就是只需要视频authorid对应的值就行了；\n",
    "      bgm_song_id：挖掘此信息是为了挖掘出用户对于 bgm_song 的偏好，挖掘过程和authorid的挖掘过程相似；\n",
    "      bgm_singer_id：挖掘此信息是为了挖掘出用户对于 bgm_singer 的偏好，挖掘过程和authorid的挖掘过程相似；\n",
    "      videoplayseconds：视频时长信息，挖掘此信息是为了挖掘出用户对于视频市场的偏好信息；\n",
    "      ·用户观看历史中，视频时间长度的均值、中位数、分位数均值（前25% 到 50%的均值）\n",
    "      ·用户历史action中，视频时长的均值、中位数、分位数均值（前25% 到 50%的均值）\n",
    "      ·和某一action强相关的action中，视频时长的均值、中位数、分位数均值（前25% 到 50%的均值）。\n",
    "      ·用户所有完整播放的视频中，视频时长的均值、中位数、分位数均值（前25% 到 50%的均值）\n",
    "      ·用户所有停留时间很长的视频中，视频时长的均值、中位数、分位数均值（前25% 到 50%的均值）\n",
    "      上述特征的值均可以和视频时长做交叉，具体思路是，上面得到的各个值和视频时长之间的差距，为了衡量，可以采用 1/exp（|得到的值-视频时长|/视频时长）) 表示相关度、之后可以分别将多个相关度相乘做交叉；最终衡量时间上的用户偏好；\n",
    "      top5_keywords和top5_keywords_weights: 挖掘此信息是为了挖掘用户对于关键词的偏好程度，也就是可以挖掘用户对于什么样关键词的视频更喜欢；\n",
    "      ·用户历史观看视频中，keywords出现的比例和的权重，由各个视频的keywords相加的到；\n",
    "      ·用户历史action视频中，keywords出现的比例和权重，有各个视频的keywords相加得到；\n",
    "      ·和某一action强相关的action中，keywords出现的比例和权重，由各个视频的keywords相加得到；\n",
    "      ·用户历史完整播放的action中，keywords出现的比例和权重，由各个视频的keywords相加得到；\n",
    "      ·用户历史停留时间长的视频中，keywords出现的比例和权重，由各个视频的keywords统计得到；\n",
    "      对于上述得到的用户兴趣，需要和视频做交叉，也就是只取视频的keywords对应出现的比例相加以及将视频keywords权重和用户兴趣权重相乘\n",
    "      再可以将所有的相关权重或者出现次数融合成一个关键词的总特征；\n",
    "      tag_list和tag_weights：挖掘此信息是为了挖掘用户对于视频标签的偏好，也就是挖掘用户喜欢什么样的视频；挖掘方式和前面关键词的挖掘类似即可；最终也需要将其和具体视频的特征做交叉；\n",
    "      feed_embedding挖掘：挖掘此信息是为了挖掘用户对于和给出视频相似的视频的偏好信息；\n",
    "      ·用户历史观看的视频embedding均值\n",
    "      ·用户历史action的视频embedding均值；\n",
    "      ·用户历史和某一action强相关的action的视频embedding均值\n",
    "      ·用户完整播放的视频embedding均值\n",
    "      ·用户停留时间长的embedding均值\n",
    "      上面得到的embedding都需要再和对应视频的embedding算欧氏距离或者是余弦相似度；\n",
    "      所有的欧氏距离或者余弦相似度可以再一次计算出feed_embedding相似度的总信息；\n",
    "      \n",
    "    2.过去7天内，视频的特征，这是和具体的用户无关的，而只是视频的特征信息，最后或者作为特征直接输入，或者和用户兴趣做一些交叉；\n",
    "      feedid：挖掘这个特征主要是挖掘某个视频在过去7天之内流行度信息等等；\n",
    "      ·过去7天之内，该视频出现的次数；\n",
    "      ·过去7天之内，该视频某一action的次数以及占其出现次数的比例；\n",
    "      ·过去7天之内，该视频在和某一action相关性最大的action次数以及占其出现次数的比例；\n",
    "      ·过去7天之内，该视频被完整播放的次数以及占其出现次数的比例；\n",
    "      authorid：挖掘这个特征的主要目的是挖掘视频作者的受欢迎程度；\n",
    "      ·过去7天之内，该视频作者的作品出现的次数；\n",
    "      ·过去7天之内，该视频作者的作品被action的次数，以及被action所占的比例；\n",
    "      ·过去7天之内，该视频作者的作品被完整播放的次数，以及被完整播放的比例；\n",
    "      bgm_song_id：挖掘该特征的目的是挖掘视频bgm的受欢迎程度等等，和authorid的处理比较类似；\n",
    "      ·过去7天之内，该bgm_song_id出现的次数；\n",
    "      ·过去7天之内，该bgm_song_id的作品被action的次数，以及被action所占的比例；\n",
    "      ·过去7天之内，该bgm_song_id的视频被完整播放的次数，以及完整播放所占的比例；\n",
    "      bgm_singer_id：挖掘该特征的目的是挖掘 bgm_singer 的受欢迎程度；\n",
    "      ·过去7天之内，该bgm_singer_id出现的次数；\n",
    "      ·过去7天之内，该bgm_singer_id的作品被action的次数，以及被action所占的比例；\n",
    "      ·过去7天之内，该bgm_singer_id的视频被完整播放的次数以及完整播放所占的比例；\n",
    "\n",
    "    3.其他信息的一些挖掘；\n",
    "      用户的历史行为习惯，主要是用户的历史行为action占用户所有用户观看历史的比例、以及用户完整播放视频所占的比例等；\n",
    "      视频发布时间信息，默认视频第一次出现的时间是其发布时间，可以加上视频发布时间距离最近的时间；\n",
    "      device信息的挖掘，包括不同的action中，各种device所占的比例，或者不同device的action所占action的总数比例；以及用户不同的action中，device所占的比例等；\n",
    "\n",
    "    4.在之前的特征基础上所做的交叉等等；\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ij8nvEyUO5ha"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vbCQO_VZdTI1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import os\n",
    "from scipy.spatial import distance\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HJl9l_N5cwjc"
   },
   "outputs": [],
   "source": [
    "user_action_path = \"./wechat_algo_data1/user_action.csv\"\n",
    "feed_info_modified_path = \"./feed_info_modified2.pkl\"\n",
    "test_data_path = \"./wechat_algo_data1/test_a.csv\"\n",
    "user_interest_path = \"./user_interest.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0wDsnjKAdsDt"
   },
   "outputs": [],
   "source": [
    "LABEL_COLUMNS = ['click_avatar', 'forward', 'follow', 'favorite', 'read_comment', 'comment', 'like']\n",
    "ACTION_LIST = ['click_avatar', 'forward', 'follow', 'favorite', 'read_comment', 'comment', 'like', 'is_stay', 'is_finished', 'feedback', 'interested']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9Mm1WOt_1kWf"
   },
   "outputs": [],
   "source": [
    "with open(feed_info_modified_path, 'rb') as file:\n",
    "\tfeed_info_modified_df = pickle.load(file)\n",
    " \n",
    "with open(user_interest_path, 'rb') as file:\n",
    "  user_interest_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YB93hujpeOvd"
   },
   "outputs": [],
   "source": [
    "# 读取用户历史行为数据\n",
    "user_action_df = pd.read_csv(user_action_path)\n",
    "# 读取测试集数据\n",
    "test_data_df = pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5hE9ybDOfxp3"
   },
   "outputs": [],
   "source": [
    "#统计视频第一次出现的日期，作为视频的发布日期\n",
    "feed_release_date = user_action_df.groupby(['feedid'])['date_'].apply(lambda x: x.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "EZtgaMeogJrc"
   },
   "outputs": [],
   "source": [
    "feed_release_date=pd.DataFrame(feed_release_date)\n",
    "feed_info_modified_df = pd.merge(feed_info_modified_df, feed_release_date, how='left', on='feedid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nub4uz6XgDkT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['feedid', 'authorid', 'videoplayseconds', 'bgm_song_id',\n",
      "       'bgm_singer_id', 'author_n_feeds', 'videolength_bucket',\n",
      "       'feed_embedding', 'tag_list', 'keyword_list', 'embedding_0',\n",
      "       'embedding_1', 'embedding_2', 'embedding_3', 'embedding_4',\n",
      "       'embedding_5', 'embedding_6', 'embedding_7', 'embedding_8',\n",
      "       'embedding_9', 'embedding_10', 'embedding_11', 'embedding_12',\n",
      "       'embedding_13', 'embedding_14', 'embedding_15', 'embedding_16',\n",
      "       'embedding_17', 'embedding_18', 'embedding_19', 'embedding_20',\n",
      "       'embedding_21', 'embedding_22', 'embedding_23', 'embedding_24',\n",
      "       'embedding_25', 'embedding_26', 'embedding_27', 'embedding_28',\n",
      "       'embedding_29', 'embedding_30', 'embedding_31', 'author_n_feeds_bucket',\n",
      "       'release_date_'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#将没有在之前出现过的视频发布日期设置为15\n",
    "feed_info_modified_df['date_'].fillna(15, inplace=True)\n",
    "feed_info_modified_df.rename(columns={'date_': 'release_date_'}, inplace=True)\n",
    "print(feed_info_modified_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SD_No102la_d"
   },
   "outputs": [],
   "source": [
    "feed_info_df = feed_info_modified_df.copy()\n",
    "feed_info_df.set_index(['feedid'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "et4kljA8gnhH"
   },
   "outputs": [],
   "source": [
    "#统一play和stay的单位：\n",
    "user_action_df['play'] = user_action_df['play'] / 1000.0\n",
    "user_action_df['stay'] = user_action_df['stay'] / 1000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5Blj2vN_gq5F"
   },
   "outputs": [],
   "source": [
    "#标记用户是否产生过反馈：\n",
    "import numpy as np\n",
    "\n",
    "user_action_df['feedback'] = (user_action_df[LABEL_COLUMNS].sum(axis=1) > 0).astype(np.int)\n",
    "user_action_df['interested'] = (user_action_df[['favorite', 'read_comment', 'comment', 'like']].sum(axis=1) > 0).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['userid', 'hist_tag', 'hist_keywords', 'hist_keywords_weights',\n",
      "       'hist_tag_weights'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(user_interest_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "7h041xB638D3"
   },
   "outputs": [],
   "source": [
    "user_interest_df.rename(columns={'stay': 'stay_sum'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "gV8RASUx3qWG"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['watch_sum' 'watch_sum_bucket' 'stay_sum' 'stay_sum_bucket'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-c9cbd91118ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0muser_hist_action_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muser_interest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'userid'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'watch_sum'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'watch_sum_bucket'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'stay_sum'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'stay_sum_bucket'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\anacanda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2677\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2678\u001b[0m             \u001b[1;31m# either boolean or fancy integer index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2679\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2680\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2681\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anacanda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2721\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2722\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2723\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2724\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2725\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anacanda\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[1;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[0;32m   1325\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m                     raise KeyError('{mask} not in index'\n\u001b[1;32m-> 1327\u001b[1;33m                                    .format(mask=objarr[mask]))\n\u001b[0m\u001b[0;32m   1328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['watch_sum' 'watch_sum_bucket' 'stay_sum' 'stay_sum_bucket'] not in index\""
     ]
    }
   ],
   "source": [
    "user_hist_action_df = user_interest_df[['userid', 'watch_sum', 'watch_sum_bucket', 'stay_sum', 'stay_sum_bucket']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "8lA8VwRt4YoG"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'user_hist_action_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-e9d4f1424581>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0muser_action_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_action_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_hist_action_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'userid'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'user_hist_action_df' is not defined"
     ]
    }
   ],
   "source": [
    "user_action_df = pd.merge(user_action_df, user_hist_action_df, on='userid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "nmh5tevr47bf"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'user_hist_action_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-d7d80ccee05c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0muser_hist_action_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'user_hist_action_df' is not defined"
     ]
    }
   ],
   "source": [
    "del user_hist_action_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "N4tNkN1p3iY-"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"labels ['watch_sum' 'watch_sum_bucket' 'stay_sum' 'stay_sum_bucket'] not contained in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-f644e90b1611>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0muser_interest_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'watch_sum'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'watch_sum_bucket'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'stay_sum'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'stay_sum_bucket'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\anacanda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3692\u001b[0m                                            \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3693\u001b[0m                                            \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3694\u001b[1;33m                                            errors=errors)\n\u001b[0m\u001b[0;32m   3695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3696\u001b[0m     @rewrite_axis_style_signature('mapper', [('copy', True),\n",
      "\u001b[1;32mD:\\anacanda\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3106\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3107\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3108\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3110\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anacanda\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3138\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3139\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3140\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3141\u001b[0m             \u001b[0mdropped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3142\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anacanda\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   4385\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'ignore'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4386\u001b[0m                 raise KeyError(\n\u001b[1;32m-> 4387\u001b[1;33m                     'labels %s not contained in axis' % labels[mask])\n\u001b[0m\u001b[0;32m   4388\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4389\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"labels ['watch_sum' 'watch_sum_bucket' 'stay_sum' 'stay_sum_bucket'] not contained in axis\""
     ]
    }
   ],
   "source": [
    "user_interest_df.drop(columns=['watch_sum', 'watch_sum_bucket', 'stay_sum', 'stay_sum_bucket'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "o4v7dTcEUaAn"
   },
   "outputs": [],
   "source": [
    "del user_interest_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "E0TLR0HqBIF4"
   },
   "outputs": [],
   "source": [
    "feed_info_modified_df.drop(columns=['bgm_song_id', 'bgm_singer_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "SpD-iu2ohIJc"
   },
   "outputs": [],
   "source": [
    "# 用户兴趣挖掘中三种 id 类特征挖掘可以采用完全相同的方式：\n",
    "def User_Persona_ID(user_action_df, id, action_list):\n",
    "  user_id_info = user_action_df.groupby(['userid', id]).size().reset_index()\n",
    "  user_watch_sum = user_action_df.groupby('userid').size().reset_index()\n",
    "  user_id_info = pd.merge(user_id_info, user_watch_sum, on='userid', how='left')\n",
    "  user_id_info.rename(columns={'0_x': id + '_watched', '0_y': 'watched_sum'}, inplace=True)\n",
    "  user_id_info[id + '_watched_rate'] = user_id_info[id + '_watched'] / user_id_info['watched_sum']\n",
    "  for action in action_list:\n",
    "    user_id_action = user_action_df[user_action_df[action] == 1].groupby(['userid', id]).size().reset_index()\n",
    "    user_action_sum = user_action_df[user_action_df[action] == 1].groupby('userid').size().reset_index()\n",
    "    user_id_action = pd.merge(user_id_action, user_action_sum, on='userid')\n",
    "    user_id_action.rename(columns={'0_x': id + '_' + action, '0_y': id + '_' + action + '_sum'}, inplace=True)\n",
    "    user_id_action[id + '_' + action + '_rate'] = user_id_action[id + '_' + action] / user_id_action[id + '_' + action + '_sum']\n",
    "    user_id_info = pd.merge(user_id_info, user_id_action, on=['userid', id], how='left')\n",
    "    user_id_info[id + '_' + action + '_partition'] = user_id_info[id + '_' + action] / user_id_info[id + '_watched']\n",
    "  return user_id_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ZQdeKu7yhLEm"
   },
   "outputs": [],
   "source": [
    "#对用户历史观看视频的长度进行挖掘，分别挖掘出：用户观看过、用户有过action反馈的视频时间长短的均值和中位数\n",
    "def User_Persona_Videoplayseconds(user_action_df, action_list):\n",
    "  user_videoplayseconds_info = user_action_df.groupby('userid')['videoplayseconds'].mean().reset_index()\n",
    "  user_videoplayseconds_info.rename(columns={'videoplayseconds': 'watch_videoplayseconds_mean'}, inplace=True)\n",
    "  user_videoplayseconds_info = pd.merge(user_videoplayseconds_info, \n",
    "                      user_action_df.groupby('userid')['videoplayseconds'].median().reset_index(), \n",
    "                      on='userid', \n",
    "                      how='left')\n",
    "  user_videoplayseconds_info.rename(columns={'videoplayseconds': 'watch_videoplayseconds_median'}, inplace=True)\n",
    "  for action in action_list:\n",
    "    user_videoplayseconds_info = pd.merge(user_videoplayseconds_info, \n",
    "                      user_action_df[user_action_df[action] == 1].groupby('userid')['videoplayseconds'].mean().reset_index(), \n",
    "                      on='userid', \n",
    "                      how='left')\n",
    "    user_videoplayseconds_info.rename(columns={'videoplayseconds': action + '_videoplayseconds_mean'}, inplace=True)\n",
    "    user_videoplayseconds_info = pd.merge(user_videoplayseconds_info, \n",
    "                      user_action_df[user_action_df[action] == 1].groupby('userid')['videoplayseconds'].median().reset_index(), \n",
    "                      on='userid', \n",
    "                      how='left')\n",
    "    user_videoplayseconds_info.rename(columns={'videoplayseconds': action + '_videoplayseconds_median'}, inplace=True)\n",
    "  return user_videoplayseconds_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "JIsWYL_HNcOw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anacanda\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "def create_feed_keyword_profile(user_hist_info):\n",
    "  dataset = user_hist_info['keyword_list'].values\n",
    "  from gensim.corpora import Dictionary\n",
    "  dct = Dictionary(dataset)\n",
    "  corpus = [dct.doc2bow(line) for line in dataset]\n",
    "  model = TfidfModel(corpus)\n",
    "  _feed_keywords = []\n",
    "  _feed_keywords_weights = []\n",
    "  for i in range(len(corpus)):\n",
    "    vector = model[corpus[i]]\n",
    "    feed_keywords = sorted(vector, key=lambda x: x[1], reverse=True)\n",
    "    keywords_weights = dict(map(lambda x: (dct[x[0]], x[1]), feed_keywords))\n",
    "    keywords = [i[0] for i in keywords_weights.items()]\n",
    "    _feed_keywords.append(keywords)\n",
    "    _feed_keywords_weights.append(keywords_weights)\n",
    "  return _feed_keywords, _feed_keywords_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "wWnonwq-NcUp"
   },
   "outputs": [],
   "source": [
    "# 分别计算feed的各个tag_list的权重\n",
    "def create_feed_tags_profile(user_hist_info):\n",
    "  dataset = user_hist_info['tag_list'].values\n",
    "  from gensim.corpora import Dictionary\n",
    "  dct = Dictionary(dataset)\n",
    "  corpus = [dct.doc2bow(line) for line in dataset]\n",
    "  model = TfidfModel(corpus)\n",
    "  _tags = []\n",
    "  _tags_weights = []\n",
    "  for i in range(len(corpus)):\n",
    "    vector = model[corpus[i]]\n",
    "    feed_tags = sorted(vector, key=lambda x: x[1], reverse=True)\n",
    "    tags_weights = dict(map(lambda x: (dct[x[0]], x[1]), feed_tags))\n",
    "    tags = [i[0] for i in tags_weights.items()]\n",
    "    _tags.append(tags)\n",
    "    _tags_weights.append(tags_weights)\n",
    "  return _tags, _tags_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "8cXwgITjQJAm"
   },
   "outputs": [],
   "source": [
    "unique_user_df = pd.DataFrame(user_action_df['userid'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "xvNzWkjqNhaW"
   },
   "outputs": [],
   "source": [
    "def User_Hist_Interest(split_user_action_df):\n",
    "  user_interested_df = split_user_action_df[split_user_action_df['feedback'] > 0].groupby('userid')[['keyword_list', 'tag_list']].sum().reset_index()\n",
    "  _feed_keywords, _feed_keywords_weights = create_feed_keyword_profile(user_interested_df)\n",
    "  user_interested_df['hist_keywords'] = _feed_keywords\n",
    "  user_interested_df['hist_keywords_weights'] = _feed_keywords_weights\n",
    "  tags, tags_weights = create_feed_tags_profile(user_interested_df)\n",
    "  user_interested_df['hist_tag'] = tags\n",
    "  user_interested_df['hist_tag_weights'] = tags_weights\n",
    "  user_interested_df.drop(columns=['keyword_list', 'tag_list'], inplace=True)\n",
    "  user_interested_df = pd.merge(unique_user_df, user_interested_df, on='userid', how='left')\n",
    "  user_interested_df['hist_tag'] = user_interested_df['hist_tag'].apply(lambda x: x if type(x) != float else [])\n",
    "  user_interested_df['hist_keywords'] = user_interested_df['hist_keywords'].apply(lambda x: x if type(x) != float else [])\n",
    "  user_interested_df['hist_keywords_weights'] = user_interested_df['hist_keywords_weights'].apply(lambda x: x if type(x) != float else {})\n",
    "  user_interested_df['hist_tag_weights'] = user_interested_df['hist_tag_weights'].apply(lambda x: x if type(x) != float else {})\n",
    "  return user_interested_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "HKoqQhvuhlTN"
   },
   "outputs": [],
   "source": [
    "#挖掘用户历史的 embedding 信息\n",
    "def User_Persona_Embedding(user_action_df, action_list):\n",
    "  def feed_embedding_mean(feedid):\n",
    "    return feed_info_df.loc[feedid]['feed_embedding'].mean()\n",
    "  user_embedding_df = user_action_df.groupby('userid')['feedid'].unique().reset_index()\n",
    "  user_embedding_df['feed_embedding'] = user_embedding_df['feedid'].apply(lambda x: feed_embedding_mean(x))\n",
    "  user_embedding_df.rename(columns={'feed_embedding': 'watch_embedding'}, inplace=True)\n",
    "  for action in action_list:\n",
    "    user_action_embedding_df = user_action_df[user_action_df[action] == 1].groupby('userid')['feedid'].unique().reset_index()\n",
    "    user_action_embedding_df['feed_embedding'] = user_action_embedding_df['feedid'].apply(lambda x: feed_embedding_mean(x))\n",
    "    user_action_embedding_df.drop(columns=['feedid'], inplace=True)\n",
    "    user_embedding_df = pd.merge(user_embedding_df, user_action_embedding_df, on='userid', how='left')\n",
    "    user_embedding_df.rename(columns={'feed_embedding': action + '_embedding'}, inplace=True)\n",
    "  return user_embedding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "1n-4z3LLhsQ9"
   },
   "outputs": [],
   "source": [
    "#挖掘每个视频相关的id类特征的信息\n",
    "def Feed_Persona_Id(user_action_df, id, action_list):\n",
    "  id_info = user_action_df.groupby(id).size().reset_index()\n",
    "  id_info.rename(columns={0: 'feed_' + id + '_occurs_times'}, inplace=True)\n",
    "  for action in action_list:\n",
    "    id_action_info = user_action_df[user_action_df[action] == 1].groupby(id).size().reset_index()\n",
    "    id_action_info.rename(columns={0: 'feed_' + id + '_' + action + '_times'}, inplace=True)\n",
    "    # print(id_action_info.head())\n",
    "    id_info = pd.merge(id_info, id_action_info, on=id, how='left')\n",
    "    id_info['feed_' + id + '_' + action + '_rate'] = id_info['feed_' + id + '_' + action + '_times'] / id_info['feed_' + id + '_occurs_times']\n",
    "  return id_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "6iezxpuDh2G9"
   },
   "outputs": [],
   "source": [
    "# 用户行为习惯的挖掘\n",
    "def User_Habit(user_action_df, action_list):\n",
    "  user_habit_info = user_action_df.groupby('userid').size().reset_index()\n",
    "  user_habit_info.rename(columns={0: 'past_watch_sum'}, inplace=True)\n",
    "  for action in action_list:\n",
    "    user_action = user_action_df[user_action_df[action] == 1].groupby('userid').size().reset_index()\n",
    "    user_action.rename(columns={0: action + '_sum'}, inplace=True)\n",
    "    user_habit_info = pd.merge(user_habit_info, user_action, on='userid', how='left')\n",
    "    user_habit_info[action + '_rate'] = user_habit_info[action + '_sum'] / user_habit_info['past_watch_sum']\n",
    "  return user_habit_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "xQqZFrRfh4y1"
   },
   "outputs": [],
   "source": [
    "#device信息的挖掘\n",
    "def Device_Info(user_action_df, action_list):\n",
    "  device_info = user_action_df.groupby('device').size().reset_index()\n",
    "  device_info.rename(columns={0: 'device_sum'}, inplace=True)\n",
    "  for action in action_list:\n",
    "    device_action = user_action_df[user_action_df[action] == 1].groupby('device').size().reset_index()\n",
    "    device_action.rename(columns={0: 'device_' + action + '_sum'}, inplace=True)\n",
    "    device_info = pd.merge(device_info, device_action, on='device', how='left')\n",
    "    device_info['device_' + action + '_rate'] = device_info['device_' + action + '_sum'] / device_info['device_sum']\n",
    "  return device_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "wHo0p_sEh_md"
   },
   "outputs": [],
   "source": [
    "#处理用户历史行为数据：\n",
    "def ProcessingUserAction(user_action_test, feed_info_modified_df):\n",
    "  user_action_test = pd.merge(user_action_test, feed_info_modified_df, on='feedid')\n",
    "  user_action_test['is_finished'] = (user_action_test['play'] >= user_action_test['videoplayseconds']).astype(np.int)\n",
    "  user_action_test['is_stay'] = (user_action_test['stay'] >= user_action_test['videoplayseconds'] * 2.0).astype(np.int)\n",
    "  user_author_info = User_Persona_ID(user_action_test, 'authorid', ACTION_LIST)\n",
    "  user_feed_info = user_action_test.groupby(['userid', 'feedid']).size().reset_index()\n",
    "  user_feed_info.rename(columns={0: 'user_feedid_occurs'}, inplace=True)\n",
    "  user_videoplayseconds_info = User_Persona_Videoplayseconds(user_action_test, ACTION_LIST)\n",
    "  user_embedding_info = User_Persona_Embedding(user_action_test, ACTION_LIST)\n",
    "  user_hist_interest_info = User_Hist_Interest(user_action_test)\n",
    "  user_embedding_info.drop(columns=['feedid'], inplace=True)\n",
    "  feedid_info = Feed_Persona_Id(user_action_test, 'feedid', ACTION_LIST)\n",
    "  feed_author_info = Feed_Persona_Id(user_action_test, 'authorid', ACTION_LIST)\n",
    "  user_habit_info = User_Habit(user_action_test, ACTION_LIST)\n",
    "  device_info = Device_Info(user_action_test, ACTION_LIST)\n",
    "  return user_feed_info, user_author_info, user_videoplayseconds_info, user_embedding_info, user_hist_interest_info, feedid_info, feed_author_info, user_habit_info, device_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "YYno9c30HrJg"
   },
   "outputs": [],
   "source": [
    "def keyword_tag_coocurrence(past_hist_list):\n",
    "  coocurrence = []\n",
    "  for past, hist in past_hist_list:\n",
    "    cor = 0\n",
    "    for word in past:\n",
    "      if word in hist:\n",
    "        cor += 1\n",
    "    coocurrence.append(cor)\n",
    "  return coocurrence\n",
    "\n",
    "\n",
    "def keyword_tag_weights(past_list_hist_weight):\n",
    "  coocurrence_weights = []\n",
    "  for past, hist_weights in past_list_hist_weight:\n",
    "    weight = 0\n",
    "    for word in past:\n",
    "      weight += hist_weights.get(word, 0)\n",
    "    coocurrence_weights.append(weight)\n",
    "  return coocurrence_weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "gc0X4Pc0nIiT"
   },
   "outputs": [],
   "source": [
    "#挖掘数据历史行为信息并将其作为下一天的特征\n",
    "def DataProcessing(user_hist_actions, user_tag_test, feed_info_modified, date):\n",
    "  user_feed_info, user_author_info, user_videoplayseconds_info, user_embedding_info, user_hist_interest_info, feedid_info, feed_author_info, user_habit_info, device_info = ProcessingUserAction(user_hist_actions, feed_info_modified)\n",
    "  user_tag_test['date_'] = date\n",
    "  user_tag_test = pd.merge(user_tag_test, feed_info_modified, on='feedid', how='left')\n",
    "  # 将 user_feed_info 拼接\n",
    "  user_tag_test = pd.merge(user_tag_test, user_feed_info, on=['userid', 'feedid'], how='left')\n",
    "  user_tag_test['user_feedid_occurs'] = user_tag_test['user_feedid_occurs'].fillna(0)\n",
    "  # 将 user_author_info 拼接，并将缺失值置为0\n",
    "  user_tag_test = pd.merge(user_tag_test, user_author_info, on=['userid', 'authorid'], how='left')\n",
    "  user_tag_test.drop(columns=['watched_sum'], inplace=True)\n",
    "  columns = list(user_author_info.columns.values)\n",
    "  for column in ['userid', 'authorid', 'watched_sum']:\n",
    "    columns.remove(column)\n",
    "  user_tag_test[columns] = user_tag_test[columns].fillna(0)\n",
    "  # 将 user_videoplayseconds_info 拼接，并转化为计算和视频长度的差距\n",
    "  user_tag_test = pd.merge(user_tag_test, user_videoplayseconds_info, on='userid', how='left')\n",
    "  columns = list(user_videoplayseconds_info.columns.values)\n",
    "  columns.remove('userid')\n",
    "  for column in columns:\n",
    "    user_tag_test[column] = 1.0 / np.exp(abs(user_tag_test[column] - user_tag_test['videoplayseconds']) / user_tag_test['videoplayseconds'])\n",
    "  # 计算用户历史关键词和视频关键词之间的关系\n",
    "  user_tag_test = pd.merge(user_tag_test, user_hist_interest_info, on='userid', how='left')\n",
    "  user_tag_test['tag_cooccurrence'] = keyword_tag_coocurrence(user_tag_test[['tag_list', 'hist_tag']].values)\n",
    "  user_tag_test['tag_cooccurrence_weights'] = keyword_tag_weights(user_tag_test[['tag_list', 'hist_tag_weights']].values)\n",
    "  user_tag_test['keyword_cooccurrence'] = keyword_tag_coocurrence(user_tag_test[['keyword_list', 'hist_keywords']].values)\n",
    "  user_tag_test['keyword_cooccurrence_weights'] = keyword_tag_weights(user_tag_test[['keyword_list', 'hist_keywords_weights']].values)\n",
    "  # 视频特征和其他特征集中处理：\n",
    "  other_id_info = [feedid_info, feed_author_info, user_habit_info, device_info]\n",
    "  columns_to_drop = ['feedid', 'authorid', 'userid', 'device']\n",
    "  for index, (id_info, column) in enumerate(zip(other_id_info, columns_to_drop)):\n",
    "    user_tag_test = pd.merge(user_tag_test, id_info, on=column, how='left')\n",
    "    columns = list(id_info.columns.values)\n",
    "    columns.remove(column)\n",
    "    user_tag_test[columns] = user_tag_test[columns].fillna(0)\n",
    "  # 处理 embedding 特征，计算相似度\n",
    "  def embedding_similarity(x):\n",
    "    \"\"\"\n",
    "    计算余弦相似度距离\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for pair in x:\n",
    "      result.append(distance.cosine(pair[0], pair[1]))\n",
    "    return result\n",
    "\n",
    "  def embedding_distance(x):\n",
    "    \"\"\"\n",
    "    计算欧氏距离\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for pair in x:\n",
    "      result.append(np.sqrt(np.sum((pair[0] - pair[1])**2)))\n",
    "    return result\n",
    "  \n",
    "  user_tag_test = pd.merge(user_tag_test, user_embedding_info, on='userid', how='left')\n",
    "  columns = list(user_embedding_info.columns.values)\n",
    "  columns.remove('userid')\n",
    "  for column in columns:\n",
    "    user_tag_test[column + '_dist'] = embedding_distance(user_tag_test[[column, 'feed_embedding']].values) \n",
    "    user_tag_test[column + '_dist'] = user_tag_test[column + '_dist'].fillna(0)\n",
    "    user_tag_test[column + '_sim'] = embedding_similarity(user_tag_test[[column, 'feed_embedding']].values)\n",
    "    user_tag_test[column + '_sim'] = user_tag_test[column + '_sim'].fillna(0)\n",
    "  # 视频热度信息（距离首次出现的时间）\n",
    "  user_tag_test['date_gap'] = user_tag_test['date_'] - user_tag_test['release_date_']\n",
    "  # 丢弃多余的行\n",
    "  drop_columns1 = list(user_embedding_info.columns)\n",
    "  drop_columns1.remove('userid')\n",
    "  drop_columns2 = list(user_hist_interest_info.columns)\n",
    "  drop_columns2.remove('userid')\n",
    "  drop_columns = ['keyword_list', 'tag_list', 'feed_embedding', 'date_']\n",
    "  drop_columns += drop_columns1\n",
    "  drop_columns += drop_columns2\n",
    "  user_tag_test.drop(columns=drop_columns, inplace=True)\n",
    "  del user_feed_info, user_author_info, user_videoplayseconds_info, user_embedding_info, user_hist_interest_info, feedid_info, feed_author_info, user_habit_info, device_info\n",
    "  return user_tag_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VLfqpFuO4BD"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "YOBb0gNcLvic"
   },
   "outputs": [],
   "source": [
    "user_test_action_df = user_action_df[(user_action_df['date_'] >= 7) & (user_action_df['date_'] <= 13)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "KVGZ0q0lET4a"
   },
   "outputs": [],
   "source": [
    "user_tag_test_df = user_action_df[user_action_df['date_'] == 14][['userid', 'feedid', 'device', 'date_']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "azsGcpObiu-2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anacanda\\lib\\site-packages\\ipykernel_launcher.py:21: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "processed_data = DataProcessing(user_test_action_df, user_tag_test_df, feed_info_modified_df, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "HYBg-ppTqEJN"
   },
   "outputs": [],
   "source": [
    "def Split_and_Process(user_action_df, days): \n",
    "  \"\"\"\n",
    "  days: 滑动窗口的大小\n",
    "  \"\"\"\n",
    "  for date in range(14 - days):\n",
    "    user_action_train_split = user_action_df[(user_action_df['date_'] >= 1) & (user_action_df['date_'] <= date + days)]\n",
    "    user_action_val_split = user_action_df[user_action_df['date_'] == date + days + 1][['userid', 'feedid', 'device'] + LABEL_COLUMNS]\n",
    "    user_action_train_processed = DataProcessing(user_action_train_split, user_action_val_split, feed_info_modified_df, date + days + 1)\n",
    "    user_action_train_processed.to_csv(os.path.join(\"./gbdt_processed_data\", \"user_action_\" + str(date + days + 1) + '_version2.csv'))\n",
    "    del user_action_train_split, user_action_val_split, user_action_train_processed\n",
    "  user_action_train_split = user_action_df[(user_action_df['date_'] >= 1) & (user_action_df['date_'] <= 14)].copy()\n",
    "  user_action_val_split = test_data_df\n",
    "  user_action_train_processed = DataProcessing(user_action_train_split, user_action_val_split, feed_info_modified_df, 15)\n",
    "  user_action_train_processed.to_csv(os.path.join(\"./gbdt_processed_data\", \"test_version2.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "xxeL4niIa7G7"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "can not merge DataFrame with instance of type <class 'pandas.core.series.Series'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-2e9fff4fc134>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mSplit_and_Process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_action_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-53-84f4f2a514ef>\u001b[0m in \u001b[0;36mSplit_and_Process\u001b[1;34m(user_action_df, days)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0muser_action_train_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muser_action_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_action_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date_'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0muser_action_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date_'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0muser_action_val_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muser_action_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muser_action_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date_'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdays\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'userid'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'feedid'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'device'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mLABEL_COLUMNS\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0muser_action_train_processed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataProcessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_action_train_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_action_val_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_info_modified_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdays\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0muser_action_train_processed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./gbdt_processed_data\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"user_action_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdays\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_version2.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0muser_action_train_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_action_val_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_action_train_processed\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-48-7271c2eb8c96>\u001b[0m in \u001b[0;36mDataProcessing\u001b[1;34m(user_hist_actions, user_tag_test, feed_info_modified, date)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#挖掘数据历史行为信息并将其作为下一天的特征\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mDataProcessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_hist_actions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_tag_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_info_modified\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m   \u001b[0muser_feed_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_author_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_videoplayseconds_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_embedding_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_hist_interest_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeedid_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_author_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_habit_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mProcessingUserAction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_hist_actions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_info_modified\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m   \u001b[0muser_tag_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date_'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[0muser_tag_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_tag_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_info_modified\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'feedid'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-46-ed35a18fd617>\u001b[0m in \u001b[0;36mProcessingUserAction\u001b[1;34m(user_action_test, feed_info_modified_df)\u001b[0m\n\u001b[0;32m      9\u001b[0m   \u001b[0muser_videoplayseconds_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUser_Persona_Videoplayseconds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_action_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mACTION_LIST\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m   \u001b[0muser_embedding_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUser_Persona_Embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_action_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mACTION_LIST\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m   \u001b[0muser_hist_interest_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUser_Hist_Interest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_action_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m   \u001b[0muser_embedding_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'feedid'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m   \u001b[0mfeedid_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFeed_Persona_Id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_action_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'feedid'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mACTION_LIST\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-41-564a0e894c6a>\u001b[0m in \u001b[0;36mUser_Hist_Interest\u001b[1;34m(split_user_action_df)\u001b[0m\n\u001b[0;32m      8\u001b[0m   \u001b[0muser_interested_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hist_tag_weights'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtags_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m   \u001b[0muser_interested_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'keyword_list'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tag_list'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m   \u001b[0muser_interested_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munique_user_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_interested_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'userid'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m   \u001b[0muser_interested_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hist_tag'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muser_interested_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hist_tag'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m   \u001b[0muser_interested_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hist_keywords'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muser_interested_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hist_keywords'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anacanda\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     58\u001b[0m                          \u001b[0mright_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m                          \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindicator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m                          validate=validate)\n\u001b[0m\u001b[0;32m     61\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anacanda\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    521\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m             raise ValueError('can not merge DataFrame with instance of '\n\u001b[1;32m--> 523\u001b[1;33m                              'type {left}'.format(left=type(left)))\n\u001b[0m\u001b[0;32m    524\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m             raise ValueError('can not merge DataFrame with instance of '\n",
      "\u001b[1;31mValueError\u001b[0m: can not merge DataFrame with instance of type <class 'pandas.core.series.Series'>"
     ]
    }
   ],
   "source": [
    "Split_and_Process(user_action_df, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vr5Nb-MBSAGf"
   },
   "outputs": [],
   "source": [
    "processed_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DQ0yyeCnSBeZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPJfl5OtU/WiKgkTencR5HJ",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "mount_file_id": "1XVCQPcA4cwBTk58n30OAJTTDlySE-W1K",
   "name": "DataPreprocessing4.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
